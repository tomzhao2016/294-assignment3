{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "In this notebook, we will implement and train word2vec using the Skip-Gram model and word2vec using the CBOW model in TensorFlow. We will also investigate the behavior of pretrained GloVe embeddings models on an analogy task. This notebook is adapted from https://www.tensorflow.org/tutorials/word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "from functools import partial\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper TensorFlow functions\n",
    "from utils import get_session, maybe_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_download('http://mattmahoney.net/dc/text8.zip', 'datasets', 31344016)\n",
    "maybe_download('http://download.tensorflow.org/data/questions-words.txt', 'datasets', 603955)\n",
    "maybe_download('http://nlp.stanford.edu/data/glove.6B.zip', 'datasets', 862182613)\n",
    "if not os.path.exists(os.path.join(\"datasets\", \"glove.6B.50d.txt\")):\n",
    "    with zipfile.ZipFile(os.path.join(\"datasets\", \"glove.6B.zip\"), \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"datasets\")\n",
    "    for f in [\"glove.6B.100d.txt\", \"glove.6B.300d.txt\", \"glove.6B.200d.txt\"]:\n",
    "        os.remove(os.path.join('datasets', f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a list of strings.\n",
    "def read_vocabulary(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "vocabulary = read_vocabulary(os.path.join('datasets', 'text8.zip'))\n",
    "print('Number of vocabulary items:', len(vocabulary))\n",
    "\n",
    "# Build the dictionary and replace rare words with UNK token.\n",
    "vocab_size = 50000\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "# data - list of codes (integers from 0 to vocabulary_size-1).\n",
    "#   This is the original text but words are replaced by their codes\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# dictionary - map of words(strings) to their codes(integers)\n",
    "# reverse_dictionary - maps codes(integers) to words(strings)\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, \n",
    "                                                            vocab_size)\n",
    "del vocabulary  # To reduce memory consumption\n",
    "print('Most common words (+UNK):', count[:5])\n",
    "print('Sample data:', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **word embedding** $W: \\{1, \\dots, N\\} \\to \\mathbb{R}^n$ is a parametric function that maps a word type to a high-dimensional vector. Typically, the function is a simple lookup table parameterized by a matrix $\\mathbf{W}$, with a new row for each word type; the query $W(i)$ will return the $i$th row of $\\mathbf{W}$. As usual in the machine learning paradigm, $\\mathbf{W}$ is initialized randomly, and trained to produce small cost under some objective function.\n",
    "\n",
    "**What's the motivation for using word embeddings?** We have seen that image processing systems work with complex data encoded as vectors of pixel intensities. On the other hand, traditional NLP systems used to treat words as symbolic tokens. These symbols are arbitrary, and don't provide information about the relationships that may exist between the individual symbols.\n",
    "\n",
    "To solve these issues, vector space models represent -- or *embed* -- words into a continuous vector space that minimizes distances between semantically similar words.\n",
    "Predictive vector space models implement this principle by trying to predict a word from its neighbors (or vice versa) in terms of learned embedding vectors that are considered parameters of the model.\n",
    "\n",
    "In the following, we investigate some different ways of learning word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec with NCE\n",
    "In classic neural probabilistic  language models (and in tutorial), we use a maximum likelihood criterion to estimate the parameters of a model that computes a vector representation $u_i$ for a word $w_i$ as well as a contextual representation $v_j$ for a history $h_j$ (which could simply be the representation of the preceding word in a unigram model, or a more complex summary of the context). We assume a multinomial distribution over $w_i$, and thus the likelihood objective is a softmax function:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(w_i | h_j) &= \\frac{\\exp \\{ u_{i} \\cdot v_{j} \\} }\n",
    "             {\\sum_{k=1}^{|\\mathcal{V}|} \\exp \\{ u_{k} \\cdot v_{j} \\} }~.\n",
    "             \\tag{1}\n",
    "\\end{align}$$\n",
    "\n",
    "**But this is very expensive to compute!** At every timestep $t$ and for each word $w_i$, we need to compute the normalization and sum over words in the vocabulary $\\mathcal{V}$, which could be extremely large (often around $10^5$ to $10^7$ terms). Luckily, we can use a method called *noise-contrastive estimation* (NCE) so that we needn't use the full probabilistic model as above, but instead use a binary classification objective. The insight is to try to discriminate the target word $w_t$ from randomly sampled *noise* words $\\tilde{w} \\sim p_\\text{noise}(w)$. The noise distribution $p_\\text{noise}(w)$ is often chosen to be the uniform distribution over all words in the vocabulary.\n",
    "\n",
    "NCE can be shown to approximately maximize the log probability of the full softmax in (1); you can check out the original paper that applied NCE to general unnormalized probability models [1] and or its first application to language modeling [2]. In practice, word embedding training procedures use a slightly modified version of the NCE objective, called *negative sampling* (NS), introduced in [3], in which samples from the noise distribution are strictly *negative* examples (i.e., not $w_t$). Consider the NS objective for a single word-context pair:\n",
    "\n",
    "$$\\ell_\\text{NS} (w_i, h_j)\n",
    "= \\log p_\\theta (D=1 | w_i, h_j) +\n",
    "  k \\mathop{\\mathbb{E}}_{\\tilde w \\sim p_\\text{noise}}\n",
    "  \\left[ \\log p_\\theta(D = 0 |\\tilde w, h_j) \\right]~.$$\n",
    "\n",
    "which you can identify as logistic regression applied to the task of distinguishing the target word $w_i$ from draws from the noise distribution $p_\\text{noise}(w)$. For a more in-depth explanation, see the tutorial here: https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#noise-contrastive-estimation-nce.\n",
    "\n",
    "NS computes a binary classification with sigmoid functions as follows:\n",
    "\n",
    "$$% <![CDATA[\n",
    "\\begin{align}\n",
    "p_\\theta (D=1 | w_i, h_j) &= \\sigma({u_{i}} \\cdot v_{j}) \\\\\n",
    "p_\\theta(D = 0 |w_k, h_j) &= 1 - \\sigma({u_{k}} \\cdot v_{j}) = \\sigma(-{u_{k}}\\cdot v_{j})\n",
    "\\end{align} %]]>~.$$\n",
    "\n",
    "so that the final NS loss function looks like:\n",
    "\n",
    "$$\\mathcal{L}_\\theta = - [ \\log \\sigma({u_i}\\cdot v_{j}) +  \\sum_{\\substack{k=1 \\\\ w_k \\sim p_\\text{noise}}}^N \\log \\sigma(-{u_{k}}\\cdot v_{j})]~.\\tag{2}$$\n",
    "\n",
    "**For the word2vec models below, implement the NCE/NS objective (2) instead of the full probabilistic model (1).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.A word2vec with CBOW\n",
    "First off, we'll consider the Continuous Bag-of-Words (CBOW) model of word embeddings (introduced in Section 3.1 in Mikolov et al.). CBOW predicts a target word from its source context words (for example \"in the hat\" predicts \"cat\"). Since the source context is treated as a single observation, CBOW tends to neglect specific cooccurences of word types (in contrast to the Skip-Gram model, as we will discuss later).\n",
    "\n",
    "**For this question, implement the computational graph that computes the NCE objective for the word2vec CBOW model. Do NOT make use of higher-level primitives from the tf.nn module.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_data_index = 0\n",
    "\n",
    "def generate_batch_cbow(data, batch_size, context_window=1):\n",
    "    \"\"\"Generate a batch of examples and targets for use in the CBOW model.\"\"\"\n",
    "    global cbow_data_index\n",
    "    context_size = 2 * context_window\n",
    "    batch = np.ndarray(shape=(batch_size, context_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * context_window + 1  # [ context_window target context_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[cbow_data_index])\n",
    "        cbow_data_index = (cbow_data_index + 1) % len(data)\n",
    "    for i in range(batch_size):\n",
    "        # context tokens are just all the tokens in buffer except the target\n",
    "        batch[i, :] = [token for idx, token in enumerate(buffer) if idx != context_window]\n",
    "        labels[i, 0] = buffer[context_window]\n",
    "        buffer.append(data[cbow_data_index])\n",
    "        cbow_data_index = (cbow_data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "def word2vec_cbow(vocab_size, embed_size, num_noise_samples=64, context_size=2, name='word2vec_cbow'):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[None, context_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        ### Make sure all variable definitions are within the scope!\n",
    "        ### Implement the NCE loss as a logistic regression objective\n",
    "        ### that discriminates noise from data samples\n",
    "        raise NotImplementedError(\"Need to implement the word2vec CBOW objective.\")\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        ### Your code should instantiate the embeddings and compute the \n",
    "        ### loss evaluated on training inputs and labels\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.gather(normalized_embeddings, valid_dataset)\n",
    "        valid_similarity = tf.matmul(valid_embeddings, \n",
    "                                     normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    return train_inputs, train_labels, loss, normalized_embeddings, valid_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.B word2vec with Skip-Grams\n",
    "Next up, we'll implement the Skip-Gram model of word embeddings (introduced in Section 3.2 in Mikolov et al.). The approach is algorithmically similar to CBOW; however, since each source context-target pair is treated as a distinct observation, the skip-gram encoding can model more fine-grained information about word cooccurences, and thus is better suited for datasets with infrequent words. However, the model is a little slower to train.\n",
    "\n",
    "**For this question, implement the computational graph that computes the NCE objective for the word2vec Skip-Gram model. Do NOT make use of higher-level primitives from the tf.nn module.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_data_index = 0\n",
    "\n",
    "def generate_batch_skipgram(data, batch_size, num_skips=2, skip_window=1):\n",
    "    \"\"\"Generate a batch of examples and targets for use in the SkipGram model.\"\"\"\n",
    "    global skipgram_data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if skipgram_data_index + span > len(data):\n",
    "        skipgram_data_index = 0\n",
    "    buffer.extend(data[skipgram_data_index:skipgram_data_index + span])\n",
    "    skipgram_data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if skipgram_data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            skipgram_data_index = span\n",
    "        else:\n",
    "            buffer.append(data[skipgram_data_index])\n",
    "            skipgram_data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    skipgram_data_index = (skipgram_data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "def word2vec_skipgram(vocab_size, embed_size, num_noise_samples=64, name='word2vec_skipgram'):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        ### Make sure all variable definitions are within the scope!\n",
    "        ### Implement the NCE loss as a logistic regression objective\n",
    "        ### that discriminates noise from data samples\n",
    "        raise NotImplementedError(\"Need to implement the word2vec Skip-Gram objective.\")\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        ### Your code should instantiate the embeddings and compute the \n",
    "        ### loss evaluated on training inputs and labels\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.gather(normalized_embeddings, valid_dataset)\n",
    "        valid_similarity = tf.matmul(valid_embeddings, \n",
    "                                     normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    return train_inputs, train_labels, loss, normalized_embeddings, valid_similarity  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "You can test your implementations by checking if the loss decreases over training, looking at the similarity predictions on the validation set, and also visualizing the t-SNE embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw visualization of distance between embeddings.\n",
    "def plot_with_labels(low_dim_embs, labels):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            ha='right',\n",
    "            va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "def train_embedding_model(train_inputs, \n",
    "                          train_labels, \n",
    "                          loss, \n",
    "                          normalized_embeddings, \n",
    "                          validation_similarities, \n",
    "                          generate_batch_fn, \n",
    "                          num_steps):\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0\n",
    "    # You may find that tuning the learning rate is helpful\n",
    "    optimize_op = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Add variable initializer\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with get_session() as session:\n",
    "\n",
    "        init.run()\n",
    "        print('Initialized the computational graph.')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "\n",
    "            batch_inputs, batch_labels = generate_batch_fn()\n",
    "            feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "                \n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            _, loss_val = session.run([optimize_op, loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive! (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0:\n",
    "                sim = validation_similarities.eval()\n",
    "                for i in xrange(valid_size):\n",
    "                    valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                    top_k = 8  # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    for k in xrange(top_k):\n",
    "                        close_word = reverse_dictionary[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "              \n",
    "        # Return the values of the embeddings at the end of training\n",
    "        return normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use this block to train the different models\n",
    "embed_size = 128       # embedding dimension\n",
    "batch_size = 128         # number of examples in a minibatch\n",
    "num_steps  = 100000000 # number of minibatches to observe during training\n",
    "\n",
    "valid_size = 16      # Random set of words to evaluate similarity on.\n",
    "valid_window = 100   # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "batch_function_kwargs = {'data': data, 'batch_size': batch_size}\n",
    "\n",
    "### SELECT MODEL HERE\n",
    "#train_inputs, train_labels, loss, normalized_embeddings, valid_similarity = word2vec_skipgram(vocab_size, embed_size)\n",
    "#generate_batch_fn = partial(generate_batch_skipgram, **batch_function_kwargs)\n",
    "\n",
    "#train_inputs, train_labels, loss, normalized_embeddings, valid_similarity = word2vec_cbow(vocab_size, embed_size)\n",
    "#generate_batch_fn = partial(generate_batch_cbow, **batch_function_kwargs)\n",
    "\n",
    "# Call the training loop\n",
    "final_embeddings = train_embedding_model(train_inputs, \n",
    "                                         train_labels, \n",
    "                                         loss, \n",
    "                                         normalized_embeddings, \n",
    "                                         valid_similarity, \n",
    "                                         generate_batch_fn, \n",
    "                                         num_steps)\n",
    "\n",
    "try:\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.C Compare CBOW and Skip-Gram\n",
    "\n",
    "**Did you notices any differences in training or evaluation between these two methods? Give a brief summary below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your written answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word Similarity & Analogies with Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretrained word embeddings** are word embeddings that have already been constructed in advance using some (usually task-agnostic) training procedure. The advantage of using off-the-shelf, pretrained word embeddings is that they are computationally simple to use (since no fine-tuning is required). The downside is that they may not be the best for the task we wish to apply them to. We can therefore evaluate the quality of pretrained embeddings by applying them to a specific downstream task such finding the word that is most similar to a given word, or analogical reasoning. The distance (Euclidean or cosine) between two word embeddings should measure the linguistic or semantic similarity of the corresponding words.\n",
    "\n",
    "Here, we're using GloVe embeddings https://nlp.stanford.edu/projects/glove/ [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    vocab = []\n",
    "    embed = []\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            row = line.strip().split(' ')\n",
    "            vocab.append(row[0])\n",
    "            embed.append(row[1:])\n",
    "    embed = np.asarray(embed)\n",
    "    return vocab, embed\n",
    "\n",
    "def read_analogies(analogy_filepath, word2id):\n",
    "    questions = []\n",
    "    with open(analogy_filepath, \"r\") as analogy_f:\n",
    "        for line in analogy_f:\n",
    "            if line.startswith(\":\"):  # Skip comments.\n",
    "                continue\n",
    "            words = line.strip().lower().split(\" \")\n",
    "            ids = [word2id.get(w.strip()) for w in words]\n",
    "            if not (None in ids or len(ids) != 4):\n",
    "                questions.append(np.array(ids))\n",
    "    np.random.shuffle(questions)  # randomize the analogies\n",
    "    return np.array(questions, dtype=np.int32)\n",
    "\n",
    "# Load the GloVe vectors into numpy\n",
    "glove_filepath = os.path.join('datasets', 'glove.6B.50d.txt')\n",
    "vocab, embed = load_embeddings(glove_filepath)\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = len(embed[0])\n",
    "assert vocab_size > 0, \"The vocabulary shouldn't be empty; did you download the GloVe weights?\"\n",
    "print('Loaded %d %d-dimensional embeddings.' % (vocab_size, embed_dim))\n",
    "\n",
    "word2id = {}\n",
    "id2word = vocab\n",
    "for i, w in enumerate(id2word):\n",
    "    word2id[w] = i\n",
    "\n",
    "# Load the analogies\n",
    "analogies = read_analogies(os.path.join('datasets', 'questions-words.txt'), word2id)\n",
    "assert analogies.shape[0] > 0, \"The matrix of analogies shouldn't be empty; did you download the analogies?\"\n",
    "print('Loaded %d analogies.' % analogies.shape[0])\n",
    "\n",
    "# Ops to load the embeddings into TensorFlow\n",
    "embedding = tf.Variable(tf.constant(0.0, shape=[vocab_size, embed_dim]),\n",
    "                        trainable=False, name=\"embedding\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embed_dim])\n",
    "embedding_init = embedding.assign(embedding_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.A Similar words & antonyms\n",
    "\n",
    "By learning word embeddings that encode distributional information, we can make use of the principle that *similar words appear in similar contexts*. In paritcular, similar words should have similar embeddings. **Verify this fact by finishing the implementation of a method that find the words with minimum cosine distance to the embedding of a given word in the vocabulary.**\n",
    "\n",
    "However, one known problem with word embeddings is that antonyms (words with meanings considered to be opposites) often have similar embeddings. You can verify this by searching for an antonym in the top *k* most similar words to a given word. **Find at least 3 such word-antonym pairs with similar embeddings, and give a textual justification for why this occurs in word embedding models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your written answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_pairs(embedding):\n",
    "    \n",
    "    target = tf.placeholder(dtype=tf.int32)\n",
    "    \n",
    "    ### Find the top most similar words\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    ### ... distances should a vector of size [vocab_size] containing\n",
    "    ### the distances of the target d to each word in the vocabulary\n",
    "    \n",
    "    # Return top 10 as a representative sample\n",
    "    _, top_k_idx = tf.nn.top_k(distances, 10)\n",
    "    \n",
    "    return target, top_k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime commands\n",
    "\n",
    "### Define some target words for which to look up similar words\n",
    "### YOUR CODE HERE\n",
    "targets = [...]\n",
    "### END YOUR CODE\n",
    "\n",
    "targets = [word2id[target] for target in targets]\n",
    "\n",
    "with get_session() as sess:\n",
    "    \n",
    "    # Initialize the embedding matrix\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: embed})\n",
    "\n",
    "    # Create the ops to complete the analogy\n",
    "    target_placeholder, top_idx = word_pairs(embedding)\n",
    "\n",
    "    # Complete some analogies!\n",
    "    for i, target in enumerate(targets):\n",
    "        idx = sess.run(top_idx, {target_placeholder: [target]})\n",
    "        print(\"Word #%d: %s\" % (i + 1, target))\n",
    "        print(\"\\t%s\" % ', '.join([id2word[j] for j in idx[0]]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.B Solving the analogy task\n",
    "\n",
    "\n",
    "The analogy task that makes use of the  is as follows: Given a triplet of words $w_a$, $w_b$ and $w_c$, select the appropriate word $w_d$ to complete the following analogy:\n",
    "\n",
    "<center>$w_a$ is to $w_b$ as $w_c$ is to ???.</center>\n",
    "\n",
    "An example task woud be is: \"Man is to woman as king is to ???\"; the correct answer is \"queen\". Mikolov et al. [2013a] proposed that simple algebraic operations could be applied to vector space embeddings to solve such an analogy task. For $w_d$ that completes the analogy, we expect that\n",
    "\n",
    "$$v_{w_b} - v_{w_a} \\approx v_{w_d} - v_{w_c}$$\n",
    "\n",
    "i.e., that *the difference between $w_b$ and $w_a$ is similar to the difference between $w_d$ and $w_c$*. The $w_d$ that we would like is therefore\n",
    "\n",
    "$$v_{w_d} = \\arg \\max_{w_d \\in \\mathcal{V} \\setminus \\{w_a, w_b, w_c\\}}\n",
    "\\cos \\left(v_{w_d}, v_{w_b} - v_{w_a} + v_{w_c}\\right)~. \\tag{3}$$\n",
    "\n",
    "For this question, implement the analogy prediction method described in (3) by completing the skeleton below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(embedding):\n",
    "    \n",
    "    a = tf.placeholder(dtype=tf.int32)\n",
    "    b = tf.placeholder(dtype=tf.int32)\n",
    "    c = tf.placeholder(dtype=tf.int32)\n",
    "    \n",
    "    ### We need to use the embeddings to solve the analogy task\n",
    "    ### \"a is to b as c is to d\"\n",
    "    ### for d (given a, b, and c)\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    ### ... distances should a vector of size [vocab_size] containing\n",
    "    ### the distances of the target d to each word in the vocabulary\n",
    "    \n",
    "    # Return top 4 in case we accidentally predict a, b or c\n",
    "    _, top_k_idx = tf.nn.top_k(distances, 4)\n",
    "    \n",
    "    return a, b, c, top_k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime commands\n",
    "with get_session() as sess:\n",
    "    \n",
    "    # Initialize the embedding matrix\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: embed})\n",
    "\n",
    "    # Create the ops to complete the analogy\n",
    "    a_placeholder, b_placeholder, c_placeholder, top_idx = complete_analogy(embedding)\n",
    "\n",
    "    # Complete some analogies!\n",
    "    for i in range(10):\n",
    "        w0, w1, w2, w3 = analogies[i]\n",
    "        idx, = sess.run(top_idx, {\n",
    "            a_placeholder: [w0],\n",
    "            b_placeholder: [w1],\n",
    "            c_placeholder: [w2],\n",
    "        })\n",
    "        a, b, c, d = [id2word[j] for j in [w0, w1, w2, w3]]\n",
    "        print(\"Analogy #%d:\" % (i + 1))\n",
    "        for d_hat in [id2word[j] for j in idx]:\n",
    "            if d_hat not in [a, b, c]:\n",
    "                print('\\t%s is to %s as %s is to %s\\t(ground truth: %s)' % (a, b, c, d_hat, d))\n",
    "                print('')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Gutmann, Michael, and Aapo Hyv√§rinen. \"Noise-contrastive estimation: A new estimation principle for unnormalized statistical models.\" In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 297-304. 2010. http://proceedings.mlr.press/v9/gutmann10a.html\n",
    "\n",
    "[2] Mnih, Andriy, and Yee Whye Teh. \"A fast and simple algorithm for training neural probabilistic language models.\" In Proceedings of the 29th International Conference\n",
    "on Machine Learning, Edinburgh, Scotland, UK, 2012. https://arxiv.org/abs/1206.6426\n",
    "\n",
    "[3] Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. \"Distributed representations of words and phrases and their compositionality.\" In Advances in neural information processing systems, pp. 3111-3119. 2013.\n",
    "\n",
    "[4] Pennington, Jeffrey, Richard Socher, and Christopher Manning. \"Glove: Global vectors for word representation.\" In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532-1543. 2014."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
